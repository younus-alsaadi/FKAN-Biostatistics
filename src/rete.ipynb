{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import lru_cache\n",
    "\n",
    "class KAL_Net(nn.Module):  # Kolmogorov Arnold Legendre Network (KAL-Net)\n",
    "    def __init__(self, layers_hidden, polynomial_order=3, base_activation=nn.SiLU):\n",
    "        super(KAL_Net, self).__init__()  # Initialize the parent nn.Module class\n",
    "        \n",
    "        # layers_hidden: A list of integers specifying the number of neurons in each layer\n",
    "        self.layers_hidden = layers_hidden\n",
    "        # polynomial_order: Order up to which Legendre polynomials are calculated\n",
    "        self.polynomial_order = polynomial_order\n",
    "        # base_activation: Activation function used after each layer's computation\n",
    "        self.base_activation = base_activation()\n",
    "        \n",
    "        # ParameterList for the base weights of each layer\n",
    "        self.base_weights = nn.ParameterList()\n",
    "        # ParameterList for the polynomial weights for Legendre expansion\n",
    "        self.poly_weights = nn.ParameterList()\n",
    "        # ModuleList for layer normalization for each layer's output\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "\n",
    "        # Initialize network parameters\n",
    "        for i, (in_features, out_features) in enumerate(zip(layers_hidden, layers_hidden[1:])):\n",
    "            # Base weight for linear transformation in each layer\n",
    "            self.base_weights.append(nn.Parameter(torch.randn(out_features, in_features)))\n",
    "            # Polynomial weight for handling Legendre polynomial expansions\n",
    "            self.poly_weights.append(nn.Parameter(torch.randn(out_features, in_features * (polynomial_order + 1))))\n",
    "            # Layer normalization to stabilize learning and outputs\n",
    "            self.layer_norms.append(nn.LayerNorm(out_features))\n",
    "\n",
    "        # Layer of output for binary classification (1 output unit)\n",
    "        self.output_layer = nn.Linear(layers_hidden[-1], 1)\n",
    "\n",
    "        # Initialize weights using Kaiming uniform distribution for better training start\n",
    "        for weight in self.base_weights:\n",
    "            nn.init.kaiming_uniform_(weight, nonlinearity='linear')\n",
    "        for weight in self.poly_weights:\n",
    "            nn.init.kaiming_uniform_(weight, nonlinearity='linear')\n",
    "\n",
    "    @lru_cache(maxsize=128)  # Cache to avoid recomputation of Legendre polynomials\n",
    "    def compute_legendre_polynomials(self, x, order):\n",
    "        # Base case polynomials P0 and P1\n",
    "        P0 = x.new_ones(x.shape)  # P0 = 1 for all x\n",
    "        if order == 0:\n",
    "            return P0.unsqueeze(-1)\n",
    "        P1 = x  # P1 = x\n",
    "        legendre_polys = [P0, P1]\n",
    "        \n",
    "        # Compute higher order polynomials using recurrence\n",
    "        for n in range(1, order):\n",
    "            Pn = ((2.0 * n + 1.0) * x * legendre_polys[-1] - n * legendre_polys[-2]) / (n + 1.0)\n",
    "            legendre_polys.append(Pn)\n",
    "        \n",
    "        return torch.stack(legendre_polys, dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is on the right device from the start, matching the model parameters\n",
    "        x = x.to(self.base_weights[0].device)\n",
    "\n",
    "        for i, (base_weight, poly_weight, layer_norm) in enumerate(zip(self.base_weights, self.poly_weights, self.layer_norms)):\n",
    "            # Apply base activation to input and then linear transform with base weights\n",
    "            base_output = F.linear(self.base_activation(x), base_weight)\n",
    "            \n",
    "            # Normalize x to the range [-1, 1] for stable Legendre polynomial computation\n",
    "            x_normalized = 2 * (x - x.min()) / (x.max() - x.min()) - 1\n",
    "            # Compute Legendre polynomials for the normalized x\n",
    "            legendre_basis = self.compute_legendre_polynomials(x_normalized, self.polynomial_order)\n",
    "            # Reshape legendre_basis to match the expected input dimensions for linear transformation\n",
    "            legendre_basis = legendre_basis.view(x.size(0), -1)\n",
    "\n",
    "            # Compute polynomial output using polynomial weights\n",
    "            poly_output = F.linear(legendre_basis, poly_weight)\n",
    "            # Combine base and polynomial outputs, normalize, and activate\n",
    "            x = self.base_activation(layer_norm(base_output + poly_output))\n",
    "\n",
    "        # Final output layer for binary classification with sigmoid activation\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x)  # Sigmoid to get a value between 0 and 1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
